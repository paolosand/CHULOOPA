{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drum Feature Analysis\n",
    "\n",
    "**Goal:** Analyze which features are most discriminative for kick/snare/hat classification\n",
    "\n",
    "This notebook will:\n",
    "1. Load training data\n",
    "2. Visualize feature distributions per class\n",
    "3. Identify most discriminative features\n",
    "4. Test classification accuracy with different feature subsets\n",
    "5. Recommend optimal feature set for user training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print('âœ“ Libraries loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = pd.read_csv('training_samples.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "feature_cols = [col for col in df.columns if col not in ['label', 'timestamp']]\n",
    "X = df[feature_cols].values\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Distribution Analysis\n",
    "\n",
    "Visualize how each feature separates the three drum classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions for all features\n",
    "n_features = len(feature_cols)\n",
    "n_cols = 5\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(feature_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for label in ['kick', 'snare', 'hat']:\n",
    "        data = df[df['label'] == label][feature]\n",
    "        ax.hist(data, alpha=0.5, label=label, bins=15)\n",
    "    \n",
    "    ax.set_title(feature)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('âœ“ Feature distributions plotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Feature Ranking (ANOVA F-statistic)\n",
    "\n",
    "Use ANOVA to rank features by their ability to discriminate between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ANOVA F-statistic for each feature\n",
    "f_scores = []\n",
    "p_values = []\n",
    "\n",
    "for feature in feature_cols:\n",
    "    kick_vals = df[df['label'] == 'kick'][feature]\n",
    "    snare_vals = df[df['label'] == 'snare'][feature]\n",
    "    hat_vals = df[df['label'] == 'hat'][feature]\n",
    "    \n",
    "    f_stat, p_val = f_oneway(kick_vals, snare_vals, hat_vals)\n",
    "    f_scores.append(f_stat)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "# Create ranking dataframe\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'f_score': f_scores,\n",
    "    'p_value': p_values\n",
    "})\n",
    "\n",
    "feature_ranking = feature_ranking.sort_values('f_score', ascending=False)\n",
    "feature_ranking['rank'] = range(1, len(feature_ranking) + 1)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('FEATURE RANKING (by ANOVA F-score)')\n",
    "print('='*60)\n",
    "print(feature_ranking.to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_ranking['feature'], feature_ranking['f_score'])\n",
    "plt.xlabel('ANOVA F-score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance for Drum Classification')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pairwise Feature Scatter Plots\n",
    "\n",
    "Visualize the top discriminative features in 2D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 6 features\n",
    "top_features = feature_ranking.head(6)['feature'].values\n",
    "\n",
    "print(f\"\\nTop 6 discriminative features:\")\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# Create pairplot for top features\n",
    "plot_df = df[['label'] + list(top_features)]\n",
    "sns.pairplot(plot_df, hue='label', diag_kind='kde', palette={'kick': 'red', 'snare': 'blue', 'hat': 'green'})\n",
    "plt.savefig('top_features_pairplot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('âœ“ Pairplot created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Performance vs. Number of Features\n",
    "\n",
    "Test how accuracy changes as we add features (from most to least discriminative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification with increasing number of features\n",
    "results = []\n",
    "\n",
    "for n_features in range(1, len(feature_cols) + 1):\n",
    "    # Select top N features\n",
    "    selected_features = feature_ranking.head(n_features)['feature'].values\n",
    "    X_selected = df[selected_features].values\n",
    "    \n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    \n",
    "    # Train SVM with cross-validation\n",
    "    clf = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n",
    "    cv_scores = cross_val_score(clf, X_scaled, y, cv=5)\n",
    "    \n",
    "    results.append({\n",
    "        'n_features': n_features,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'features': ', '.join(selected_features)\n",
    "    })\n",
    "    \n",
    "    if n_features <= 10:\n",
    "        print(f\"Top {n_features:2d} features: CV accuracy = {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot accuracy vs. number of features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(results_df['n_features'], results_df['cv_mean'], marker='o', linewidth=2)\n",
    "plt.fill_between(results_df['n_features'], \n",
    "                 results_df['cv_mean'] - results_df['cv_std'],\n",
    "                 results_df['cv_mean'] + results_df['cv_std'],\n",
    "                 alpha=0.3)\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Classification Accuracy vs. Number of Features')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.8, color='r', linestyle='--', label='80% accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_vs_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find optimal number of features (highest accuracy)\n",
    "best_idx = results_df['cv_mean'].idxmax()\n",
    "best_result = results_df.loc[best_idx]\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('OPTIMAL FEATURE SET')\n",
    "print('='*60)\n",
    "print(f\"Number of features: {best_result['n_features']}\")\n",
    "print(f\"CV accuracy: {best_result['cv_mean']:.3f} (+/- {best_result['cv_std']:.3f})\")\n",
    "print(f\"\\nFeatures: {best_result['features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix for Optimal Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with optimal features\n",
    "optimal_n = int(best_result['n_features'])\n",
    "optimal_features = feature_ranking.head(optimal_n)['feature'].values\n",
    "\n",
    "X_optimal = df[optimal_features].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_optimal)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "clf = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print('\\n' + '='*60)\n",
    "print('CLASSIFICATION REPORT (Test Set)')\n",
    "print('='*60)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['kick', 'snare', 'hat'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['kick', 'snare', 'hat'],\n",
    "            yticklabels=['kick', 'snare', 'hat'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title(f'Confusion Matrix (Top {optimal_n} Features)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Correlations\n",
    "\n",
    "Check if any features are highly correlated (redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for top features\n",
    "top_10_features = feature_ranking.head(10)['feature'].values\n",
    "corr_matrix = df[top_10_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix (Top 10 Features)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_correlation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "print('\\nHighly correlated feature pairs (|r| > 0.7):')\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix)):\n",
    "    for j in range(i+1, len(corr_matrix)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append((corr_matrix.index[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    for feat1, feat2, corr in high_corr_pairs:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")\n",
    "else:\n",
    "    print('  None found (good - features are independent!)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommended Feature Set for User Training\n",
    "\n",
    "Based on the analysis above, recommend a minimal feature set that:\n",
    "1. Achieves high accuracy (>80%)\n",
    "2. Uses fewest features (for faster extraction)\n",
    "3. Has low inter-feature correlation (independent information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find minimal feature set that achieves >80% accuracy\nthreshold = 0.80\nminimal_features = None\n\nfor idx, row in results_df.iterrows():\n    if row['cv_mean'] >= threshold:\n        minimal_features = row\n        break\n\nprint('\\n' + '='*70)\nprint('RECOMMENDED FEATURE SET FOR USER TRAINING')\nprint('='*70)\n\nif minimal_features is not None:\n    rec_n = int(minimal_features['n_features'])\n    rec_features = feature_ranking.head(rec_n)['feature'].values\n    \n    print(f\"\\nMinimal features for >{threshold*100:.0f}% accuracy: {rec_n} features\")\n    print(f\"Expected accuracy: {minimal_features['cv_mean']:.1%} (+/- {minimal_features['cv_std']:.1%})\")\n    print(f\"\\nRecommended features:\")\n    for i, feat in enumerate(rec_features, 1):\n        f_score = feature_ranking[feature_ranking['feature'] == feat]['f_score'].values[0]\n        print(f\"  {i}. {feat:12s} (F-score: {f_score:.2f})\")\n    \n    print(f\"\\n{'='*70}\")\n    print('IMPLEMENTATION NOTES:')\n    print('='*70)\n    print(f\"- These {rec_n} features balance accuracy and computational efficiency\")\n    print(f\"- Users only need 10 samples per class (30 total) for training\")\n    print(f\"- Real-time extraction should be <10ms with these features\")\n    print(f\"- Consider removing MFCC if computation is still too slow\")\n    \n    # Test without MFCCs\n    non_mfcc_features = [f for f in rec_features if not f.startswith('mfcc')]\n    if len(non_mfcc_features) > 0 and len(non_mfcc_features) < rec_n:\n        X_no_mfcc = df[non_mfcc_features].values\n        scaler_no_mfcc = StandardScaler()\n        X_scaled_no_mfcc = scaler_no_mfcc.fit_transform(X_no_mfcc)\n        \n        clf_no_mfcc = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n        cv_scores_no_mfcc = cross_val_score(clf_no_mfcc, X_scaled_no_mfcc, y, cv=5)\n        \n        print(f\"\\nAlternative (no MFCC): {len(non_mfcc_features)} features\")\n        print(f\"  Features: {', '.join(non_mfcc_features)}\")\n        print(f\"  Accuracy: {cv_scores_no_mfcc.mean():.1%} (+/- {cv_scores_no_mfcc.std():.1%})\")\n    elif len(non_mfcc_features) == 0:\n        # All recommended features are MFCCs - test with spectral features only\n        print(f\"\\nâš ï¸  All recommended features are MFCCs\")\n        print(f\"  Testing with spectral features only...\")\n        \n        spectral_features = ['flux', 'energy', 'band1', 'band2', 'band3', 'band4', 'band5', \n                           'centroid', 'rolloff', 'flatness', 'low_ratio', 'high_ratio']\n        available_spectral = [f for f in spectral_features if f in feature_cols]\n        \n        if len(available_spectral) > 0:\n            X_spectral = df[available_spectral].values\n            scaler_spectral = StandardScaler()\n            X_scaled_spectral = scaler_spectral.fit_transform(X_spectral)\n            \n            clf_spectral = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)\n            cv_scores_spectral = cross_val_score(clf_spectral, X_scaled_spectral, y, cv=5)\n            \n            print(f\"\\n  Spectral features only: {len(available_spectral)} features\")\n            print(f\"  Features: {', '.join(available_spectral)}\")\n            print(f\"  Accuracy: {cv_scores_spectral.mean():.1%} (+/- {cv_scores_spectral.std():.1%})\")\n            print(f\"  Note: Spectral features are faster to compute than MFCCs\")\n        \nelse:\n    print(f\"\\nWarning: Current data cannot achieve {threshold*100:.0f}% accuracy!\")\n    print(f\"Best achieved: {results_df['cv_mean'].max():.1%}\")\n    print(f\"\\nRecommendations:\")\n    print(f\"  1. Collect more training samples (aim for 30+ per class)\")\n    print(f\"  2. Ensure consistent beatbox technique\")\n    print(f\"  3. Check microphone quality and gain settings\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('ANALYSIS SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   - Kicks: {len(df[df['label']=='kick'])}\")\n",
    "print(f\"   - Snares: {len(df[df['label']=='snare'])}\")\n",
    "print(f\"   - Hats: {len(df[df['label']=='hat'])}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Best Performance:\")\n",
    "print(f\"   {best_result['n_features']} features\")\n",
    "print(f\"   {best_result['cv_mean']:.1%} accuracy (+/- {best_result['cv_std']:.1%})\")\n",
    "\n",
    "print(f\"\\nâš¡ Recommended for Production:\")\n",
    "if minimal_features is not None:\n",
    "    print(f\"   {rec_n} features\")\n",
    "    print(f\"   {minimal_features['cv_mean']:.1%} accuracy\")\n",
    "    print(f\"   Features: {', '.join(rec_features[:5])}...\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Top 3 Most Discriminative Features:\")\n",
    "for i in range(min(3, len(feature_ranking))):\n",
    "    feat = feature_ranking.iloc[i]\n",
    "    print(f\"   {i+1}. {feat['feature']:12s} (F-score: {feat['f_score']:.2f})\")\n",
    "\n",
    "print(f\"\\nâœ… Next Steps:\")\n",
    "print(f\"   1. Update feature_extraction.ck to use recommended features\")\n",
    "print(f\"   2. Update train_classifier.py to use optimal feature subset\")\n",
    "print(f\"   3. Test real-time classification latency\")\n",
    "print(f\"   4. Collect more samples if accuracy < 80%\")\n",
    "print(f\"\\n\" + '='*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}